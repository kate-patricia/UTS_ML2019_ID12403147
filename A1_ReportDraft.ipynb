{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "A1_ReportDraft.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kate-patricia/UTS_ML2019_ID12403147/blob/master/A1_ReportDraft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgzdEznIesaU",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1: Understanding the Literature\n",
        "## Review Report on 'Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality'\n",
        "### Written by Kate Moran 12403147"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB2N6VCkesaV",
        "colab_type": "text"
      },
      "source": [
        "## Introduction & Background (200 words)[270 words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh2N-G_cDePJ",
        "colab_type": "text"
      },
      "source": [
        "The paper 'Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality' was published in 1999 by Piotr Indyk and Rajeev Motwani.\n",
        "\n",
        "The paper considers the 'Nearest Neighbor Search' (NNS) Problem . Formally, the problem is defined as follows:\n",
        "> Given a set of n points P = {p1,...,pn} in some metric space X, preprocess P so as to efficiently answer queries which require finding the point in P closest to a query point q in X (Figure 1).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA62WfHaCFXO",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://media.springernature.com/lw785/springer-static/image/prt%3A978-3-319-17885-1%2F15/MediaObjects/978-3-319-17885-1_15_Part_Fig1-869_HTML.gif)\n",
        "\n",
        "FIGURE 1 Example of the Nearest Neighbor Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mctZsFyADgBh",
        "colab_type": "text"
      },
      "source": [
        "In 1999, the NNS Problem was decades old and had been referred to by many names, such as the Best Match Problem (Minsky-Papert 1969) and the Post Office Problem (Knuth 1973). In 1999, the authors considered the low-dimensional case of the NNS Problem to have been solved. However, they considered the NNS case when dealing with high dimensions to be a main issue with decades of unsatisfactory solutions. The authors refer in the title of their paper to the 'Curse of Dimensionality' - coined by Richard E. Bellman in 1957. The Curse of Dimensionality refers to the problems faced when working in high-dimensional space.\n",
        "\n",
        "To seek solutions to the NNS Problem, researchers began to relax the problem by introducing some approximation factor. This offshoot is called the 'Approximate Nearest Neighbor' Problem (ANN, not to be confused with 'Artifical Neural Networks'). By introducing approximation, this reduces the complexity of the problem. Indyk and Motwani argue that insisting on an absolute nearest neighbor is overkill and, for most practical purposes, an approximate nearest neighbor is sufficient.\n",
        "\n",
        "This paper presents the results of two algorithms developed by the authors to improve past solutions to the ANN Problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seKccQCIesaW",
        "colab_type": "text"
      },
      "source": [
        "## Content (300 words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xxeclZVesaW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttRl4EMWesaX",
        "colab_type": "text"
      },
      "source": [
        "## Innovation (300 words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebPeXJKyesaY",
        "colab_type": "text"
      },
      "source": [
        "There were two main waves of algorithms which attempted to solve the ANN Problem. The first wave occured in the early 1990s and the algorithms are described by Indyk (2018) as being 'exponential in d'. These algorithms provided no significant improvement to the brute-force algorithms.\n",
        "\n",
        "Towards the late 1990s, a second wave of algorithms occured. The paper that is the topic of this report, falls into the second wave.\n",
        "\n",
        "he second wave, in which the paper that is the topic of this report falls, was characterised by algorithms that were 'polynomial in d'.\n",
        "\n",
        "The background at the time of the work is that people understood the problem as .... The creative idea is ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQI5pC4qesaY",
        "colab_type": "text"
      },
      "source": [
        "## Technical quality (200 words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjAcojAvesaZ",
        "colab_type": "text"
      },
      "source": [
        "The technical development if of high/low quality. The authors supported their theory using ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6QTb92Fesaa",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor (200 words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWng2KYuesaa",
        "colab_type": "text"
      },
      "source": [
        "[MENTION APPLICATIONS]\n",
        "I find the proposal in the paper promising. ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv1ubyy8esab",
        "colab_type": "text"
      },
      "source": [
        "## Presentation (100 words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWKddqbAesab",
        "colab_type": "text"
      },
      "source": [
        "The overall strucutre is clear. I found reading is easy / difficult. The paper could have been more attractive if the authors had organised ... / provided ... "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lQ5Yx3Sesac",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "[SHA48][1]: Author, Title, Info\n",
        "\n",
        "[1]:https://google.com"
      ]
    }
  ]
}