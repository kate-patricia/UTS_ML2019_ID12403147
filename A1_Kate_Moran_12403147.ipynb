{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "A1_Kate_Moran_12403147.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kate-patricia/UTS_ML2019_ID12403147/blob/master/A1_Kate_Moran_12403147.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgzdEznIesaU",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1: Understanding the Literature\n",
        "## Review Report on 'Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality'\n",
        "### Written by Kate Moran 12403147"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB2N6VCkesaV",
        "colab_type": "text"
      },
      "source": [
        "## 1 Introduction & Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh2N-G_cDePJ",
        "colab_type": "text"
      },
      "source": [
        "The paper 'Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality' was published in 1999 by Piotr Indyk and Rajeev Motwani.\n",
        "\n",
        "The paper considers the 'Nearest Neighbor Search' (NNS) Problem . Formally, the problem is defined as follows:\n",
        "> Given a set of n points P = {p1,...,pn} in some metric space X, preprocess P so as to efficiently answer queries which require finding the point in P closest to a query point q in X (Figure 1).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA62WfHaCFXO",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://media.springernature.com/lw785/springer-static/image/prt%3A978-3-319-17885-1%2F15/MediaObjects/978-3-319-17885-1_15_Part_Fig1-869_HTML.gif)\n",
        "\n",
        "FIGURE 1 Example of the Nearest Neighbor Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mctZsFyADgBh",
        "colab_type": "text"
      },
      "source": [
        "At the time of publication, the authors considered the low-dimensional case of the NNS Problem to have been solved. However, they considered the NNS case when dealing with high dimensions to be a main issue with decades of unsatisfactory solutions. The authors refer in the title of their paper to the 'Curse of Dimensionality' - the problems faced when working in high-dimensional space.\n",
        "\n",
        "To seek solutions to the NNS Problem in high-dimensional space, researchers began to relax the problem by introducing some approximation factor. This offshoot is called the 'Approximate Nearest Neighbor' Problem (ANN, not to be confused with 'Artifical Neural Networks'). By introducing approximation, this reduces the complexity of the problem. Indyk and Motwani argue that insisting on an absolute nearest neighbor is overkill and, for most practical purposes, an approximate nearest neighbor is sufficient.\n",
        "\n",
        "This paper offers two tools and presents two algorithmic results that improve on past solutions to the ANN Problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seKccQCIesaW",
        "colab_type": "text"
      },
      "source": [
        "## 2 Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xxeclZVesaW",
        "colab_type": "text"
      },
      "source": [
        "This section will briefly review the four core tools and algorithms presented by the paper.\n",
        "\n",
        "The paper offers up two fundamental tools:\n",
        "1. Point Location in Equal Balls (PLEB), the ANN problem can be reduced to a new problem called the approximate PLEB problem.\n",
        "2. Ring-cover Trees, a data structure for efficient searching that allows for the ANN problem to be reduced.\n",
        "\n",
        "It also presents two algorithms to solve the problem:\n",
        "3. The Bucketing Method.\n",
        "4. Locality-Sensitive Hashing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKjpOMYaDWq2",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Point Location in Equal Balls & Ring-Cover Trees (128 words)\n",
        "\n",
        "The ANN problem must first be converted into an approximate _point location in equal balls_ (PLEB) problem. To achieve this, the authors of the paper have developed a data structure called 'Ring-Cover Trees', which allows the problem to be reduced. Figure 2 is a visual representation of this idea.\n",
        "\n",
        "In the figure, a small ball B(p,r) and large ball (B(p,(1+c)r)) are shown. If the query point is within the smallest ball (e.g. q1), then return YES and a point within the outer ring (e.g. q2). If the query point is within the large ball, but not within the smallest ball (e.g. q2), return either YES or NO. For query points outside of both balls (e.g. q3), then return NO.\n",
        "\n",
        "By relaxing the ANN problem further, the computational cost is reduced but this is at the expense of precision.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E8gzPL6CFzK",
        "colab_type": "text"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://i.ibb.co/1vw0LXk/Screen-Shot-2019-08-26-at-1-45-44-pm.png\" width=\"250\" alignment=\"center\"/>\n",
        "</div>\n",
        "FIGURE 2 Approximate Point Location in Equal Balls (PLEB) using Ring-Cover Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_r4pT4tDzOD",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Bucketing Method\n",
        "\n",
        "The Bucketing Algorithm creates a uniform grid over the entirety of P. Each ball that was identified in the PLEB reduction is matched to the grid and stored in a hash table. After the initial preprocessing is completed, the query point can be matched by computing the grid cell and then looking up that cell in the hash table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqQgVBs_YjCY",
        "colab_type": "text"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://i.ibb.co/4Zk4D6L/Screen-Shot-2019-08-27-at-2-39-31-pm.png\" width=\"250\" alignment=\"center\"/>\n",
        "<p>FIGURE 3 Example of a uniform grid used for Bucketing</p>\n",
        "<img src=\"https://i.ibb.co/2hHNKd6/Screen-Shot-2019-08-27-at-2-41-27-pm.png\" width=\"500\" alignment=\"center\"/>\n",
        "<p>FIGURE 4 Example of Hash Table generated from the balls in Figure 3</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t665GSOTD4A4",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Locality-Sensitive Hashing\n",
        "\n",
        "Locality-Sensitive Hashing is a type of hash function where hash value collisions are more likely for two input values that are close together than for those that are far apart. This is very helpful in the search for near neighbors.\n",
        "\n",
        "The algorithm works by slicing up the X space using random hyperplanes. The points that fall into the divided up areas are given the same hash value in the hash table. This process is repeated a number of times to eliminate false positives.\n",
        "\n",
        "Figure 4 provides a visual explanation of the LSH process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXcB1NtGh8q4",
        "colab_type": "text"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://i.ibb.co/3RjT9Dh/Screen-Shot-2019-08-27-at-3-23-47-pm.png\" width=\"300\" alignment=\"center\"/>\n",
        "<p>FIGURE 4 Random hyperplanes create groups for hash values (Lavenko 2014)</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttRl4EMWesaX",
        "colab_type": "text"
      },
      "source": [
        "## Innovation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebPeXJKyesaY",
        "colab_type": "text"
      },
      "source": [
        "In 1999, the NNS Problem was decades old and had been referred to by many names, such as the Best Match Problem (by Minsky-Papert in 1969) and the Post Office Problem (by Knuth in 1973). By the early 1990s, there had been significant work on the NNS problem in low-dimensions, but the authors considered prior work on the high-dimensional case to be unsatisfactory.\n",
        "\n",
        "There were two main waves of algorithms which attempted to solve the ANN problem. The first wave occured in the early 1990s and the algorithms are described by Indyk (2018) as being 'exponential in d'. These algorithms provided no significant improvement to the brute-force algorithms. In high-dimensions they were almost ineffective.\n",
        "\n",
        "Towards the late 1990s, a second wave of algorithms occured. The paper that is the topic of this report, falls into the second wave. The second wave was characterised by algorithms that were 'polynomial in d', which was signficantly better than the algorithms of the first wave.\n",
        "\n",
        "This authors of this paper presented four elements, two tools and two algorithms. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQI5pC4qesaY",
        "colab_type": "text"
      },
      "source": [
        "## Technical quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjAcojAvesaZ",
        "colab_type": "text"
      },
      "source": [
        "I believe that the technical quality of the work that has been presented in this paper is very high. The authors of the paper have been very thorough in the presentation of their proposed algorithms and have provided clear and detailed proofs of their work.\n",
        "\n",
        "The technical quality can also be established by how the methods and algorithms presented in this paper have been so readily accepted and utilised by the wider community and industry. One of the authors of the paper, Indyk, has continued to be a significant contributor to this area of research and his future work has also been of value to the community. \n",
        "\n",
        "Considering the age of this paper, almost twenty years old, the fact that the methods and algorithms are still being utilised is a testament to its quality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6QTb92Fesaa",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWng2KYuesaa",
        "colab_type": "text"
      },
      "source": [
        "The work put forward in this paper has been far reaching and invaluable for a very large range of applications - particularly applications of similarity searching in high-dimensional space. Some of these applications include:\n",
        "\n",
        "- recommender systems \n",
        "- text retrieval\n",
        "- searching of image and video databases\n",
        "- machine learning and pattern recognition\n",
        "- computer vision\n",
        "- data compression, databases and data mining\n",
        "\n",
        "The NNS and the ANN problem continue to be an area of interest and research is still being conducted in this area today. Indyk, one of the authors of this paper, is still researching this same topic over twenty years later and the area is still experiencing improvements.\n",
        "\n",
        "I think this paper has the x-factor! The work published by Indyk and Motwani in this paper has had a considerable and ongoing impact. The tools and algorithms, in particular the 'locality-sensitive hashing', they have put forward have become very popular since their publication. They are still in regular use in similarity searching applications twenty years later.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv1ubyy8esab",
        "colab_type": "text"
      },
      "source": [
        "## Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWKddqbAesab",
        "colab_type": "text"
      },
      "source": [
        "I feel the authors could have improved the presentation of their paper in a number of ways. \n",
        "\n",
        "I do not have extensive mathematical understanding, so I found the mathematical definitions and proofs difficult to follow. The majority of the paper is written using complicated notation. The authors could have considered that their audience may consist of individuals from different backgrounds and provided more explanatory notes.\n",
        "\n",
        "The paper could have also benefited from some visual elements to provide further explanation.\n",
        "\n",
        "The layout was sometimes confusing, as the paper jumped from one section to the next. The authors could have provided a clearer roadmap at the start of the paper to explain the journey."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lQ5Yx3Sesac",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "[IND98]: Indyk, P. & Motwani R. 1999, 'Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality', https://www.cs.princeton.edu/courses/archive/spr04/cos598B/bib/IndykM-curse.pdf\n",
        "\n",
        "\n",
        "Lavenko, R. 2014, 'LSH.9 Locality-sensitive hashing: how it works', online video, <https://www.youtube.com/watch?v=Arni-zkqMBA>.\n",
        "\n"
      ]
    }
  ]
}